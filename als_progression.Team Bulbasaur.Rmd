---
title: "Week 05 in Statistical machine learning in bioinformatics"
author: "Thomas Bataillon"
date: "25/02/2021"
output:
  html_document:
        theme: readable
editor_options: 
  chunk_output_type: console
---

------------------------------------------------------------------------

## ALS dataset

MULTIPLE LINEAR REGRESSION

Load Libraries

```{r}
# install.packages(c('tibble', 'dplyr', 'tidyr'))
library(tibble)
library(dplyr)
library(tidyr)
library(tidyverse)
library(car)
library(leaps)
library(glmnet)
library(randomForest)
```

Loading the data

```{r}
als <- read_rds("datasets/ALS_data_regression/ALS_progression_rate.1822x370.rds")

als_data    <- als %>% rename(response=dFRS)
als_train <- als_data %>% 
  filter(!is.na(response))
als_predict <- als_data %>% 
  filter(is.na(response))


dim(als_train)
dim(als_predict)

sum(is.na(als_predict))

```

We can make a correlation matrix to take a peek at how much correlation between the predictors we observe

```{r}
correlation_matrix<-cor(als_train)
```

> MLG to predict the response

```{r MLG, eval=FALSE, include=FALSE}

#Make model with ALL predictors (We should check which are more important)

mlg <- lm(response~.,data=als_train)
summary(mlg$coefficients)
summary(mlg)

#We observe 9 predictors which give NA, we will remove them manually from our model

mlg <- lm(response~. -Site.of.Onset.Onset..Limb.and.Bulbar -mean.height -sd.height -no.height.data -no.resp.rate.data -no.bp.diastolic.data -no.slope.fvc.liters.data -first.slope.height.date -num.slope.bp.systolic.visits,data=als_train)


#If we run vif we can see which ones we should remove. 
all_vifs<-car::vif(mlg)
signif_all <- names(all_vifs)

# Remove vars with VIF> 4 and re-build model until none of VIFs don't exceed 4.
while(any(all_vifs > 4)){
  var_with_max_vif <- names(which(all_vifs == max(all_vifs)))  # get the var with max vif
  signif_all <- signif_all[!(signif_all) %in% var_with_max_vif]  # remove
  myForm <- as.formula(paste("response ~ ", paste (signif_all, collapse=" + "), sep=""))  # new formula
  mlg <- lm(myForm, data=als_train)  # re-build model with new formula
  all_vifs <- car::vif(mlg)
}
summary(mlg)
car::vif(mlg)


predicted <- predict(object=mlg, newdata = als_predict, type = "response")

submission <- tibble(predicted)

head(submission)
```

> KNN Regression to predict the response

```{r}
set.seed(1)

knn_pred <- FNN::knn.reg(as.data.frame(als_train[-1]), 
                test=as.data.frame(als_predict[-1]), as.data.frame(als_train[1]),
                k = 10)
knn_pred

submission<-data.frame(predicted=knn_pred$pred)

```

```{r}
#A function that performs the KNN method on a range of k's

testtrn=als_train
testtst=als_train
testtst$response<-NA
test_k <- function(trn, tst, k) {
  err <- c()
  for (i in k) {
    pred = FNN::knn.reg(train = as.data.frame(trn[-1]), 
               test  = as.data.frame(tst[-1]), 
               as.data.frame(trn[-1]), 
               k     = k[i])
    err[i] <- sqrt(mean(trn$response - pred$pred))
  }
  df <- data.frame(k = k, x = 1/k, err_rate = err)
  return(df)
}


set.seed(2)
df <- test_k(testtrn, testtst, 1:70)

ggplot(data = df, aes(x = x, y = err_rate, group = 1)) +
  geom_line(color = "dodgerblue") +
  geom_point(color = "dodgerblue") +
  labs(title = "(Test) error rate", x = "1/K", y = "Error rate") +
  theme(plot.title = element_text(hjust = 0.5, size = 20))

min(df)
```

> CV to find the best K

```{r}
# Always make the same split


err <- c()

set.seed(0)

# We use 80% for training and 20% for evaluation
trainfold <- als_train %>% sample_frac(size=0.80)
testfold  <- setdiff(als_train, trainfold)

# We fit our model to the training fold
#fit <-  lm(formula  = myForm, data = trainfold)


# We predict on the test fold
predicted <- FNN::knn.reg(as.data.frame(trainfold[-1]), 
                test=as.data.frame(testfold[-1]), as.data.frame(trainfold[1]),
                k = i)

# We compare with the observed values and calculate RMSE
observed  <- testfold$response
mse       <- mean((observed-predicted$pred)^2)
rmse     <- sqrt(mse)

err[i] <- rmse

}


errortree<-data.frame(err)

testo<-data.frame(err)

test_rmse <- rmse

```

> Lasso

```{r}
grid =10^ seq (10,-2, length =100)

lasso.mod = glmnet(as.matrix(als_train[,-1]), als_train$response,  alpha=1, lambda=grid)

lasso.mod %>% plot()

#Cross Validation to find best lambda

set.seed(1)

cv.out=cv.glmnet(as.matrix(als_train[,-1]), als_train$response,  alpha=1)
bestlam=cv.out$lambda.min

cv.out %>% plot()

#Predict the data using our model and the best lambda we found using CV
data_testM<-as.matrix(als_predict[-1])
lasso.pred=predict(lasso.mod,s=bestlam,data_testM)

```

```{r}
cross_validation <- function(dataset, k) {
  
  cvfolds <- cut(seq_len(nrow(dataset)), breaks = k, labels = F)  # Generate breaks
  cvfolds <- sample(cvfolds)                                      # Randomize breaks
  observed  <- dataset$response                                   # Get observed values
  predicted <- rep(NA, nrow(dataset))                             # Empty vector for predicted values
  bestlam <- rep(NA, k)
  
  for (i in 1:k){                                                 # For each fold
    rows      <- which(cvfolds==i)                                # Get values belonging to that fold
    testdata  <- dataset[rows,]                                   # Get test values
    traindata <- dataset[-rows,]                                  # Get training values
    
    y <- traindata$response                                       # Get response values for training data
    traindata <- model.matrix(response ~ ., traindata)[,-1]       
    testdata <- model.matrix(response ~ ., testdata)[,-1]         # glmnet can only take quantitative input.
                                                                  #model.matrix produces a matrix of the predictors 
                                                                  #and transforms qualitative variables into 
                                                                  #dummy variables
    
    cv.out=cv.glmnet(traindata, y,  alpha=1)
    bestlam=cv.out$lambda.min
    
    
    lasso.fit = glmnet(traindata, y,  alpha=1, lambda=bestlam)
    
    lasso.pred=predict(lasso.fit,s=bestlam,testdata)
    
    
    predicted[rows] <- lasso.pred                                 # Add predictions to vector
    
  }
  mse       <- mean((observed-predicted)^2)
  (rmse     <- sqrt(mse))  # Calculate test error
  
  return(rmse)                                        # Return CV test error
}
```

# Calculate the CV test error for lasso

```{r}
set.seed(0)
lasso_test_error <- cross_validation(als_train, 10) #select number of folds (5 or 10 maybe)
```



> Cross validation for LASSO RMSE

```{r}
# Always make the same split
set.seed(0)

# We use 80% for training and 20% for evaluation
trainfold <- als_train %>% sample_frac(size=0.80)
testfold  <- setdiff(als_train, trainfold)

#Build model
lasso.fit = glmnet(as.matrix(trainfold[,-1]), trainfold$response,  alpha=1)


#Find best lambda and fit model
cv.out = cv.glmnet(as.matrix(trainfold[,-1]), trainfold$response,  alpha=1)
bestlam=cv.out$lambda.min

# We predict on the test fold
predicted <- predict(lasso.fit,s=bestlam,as.matrix(testfold[-1]))

# We compare with the observed values and calculate RMSE
observed  <- testfold$response
mse       <- mean((observed-predicted)^2)
(rmse     <- sqrt(mse))

test_rmse <- rmse
```

```{r}
set.seed(1)
model1 <- randomForest(response ~., data=als_train, ntree=150, importance=TRUE, do.trace=TRUE)
model1
importance(model1)
model1$mse
MSE <- model1$mse
MSE <- data.frame(MSE)
tree <- data.frame(n_tree = seq(1, 150, 1),
                   MSE = MSE$MSE)
ggplot(tree, aes(x=n_tree, y=MSE)) +
  geom_line() +
  labs(x = "n_trees",
       y = "MSE") +
  NULL
```

# Random forests test:

model2

```{r}
set.seed(1)
library(gbm)
set.seed(6)
model4 <- gbm(myForm, 
               data = als_train, 
            n.trees = 1000, 
           cv.folds = 10)
model4


model2 <- randomForest(response ~., data=trainfold, ntree=500, mtry = 200, importance=TRUE, do.trace=TRUE)
model2

```

```{r}
# Always make the same split
set.seed(10)

# We use 80% for training and 20% for evaluation
trainfold <- als_train %>% sample_frac(size=0.80)
testfold  <- setdiff(als_train, trainfold)

# We fit our model using trainfold

model2 <- randomForest(response ~., data=trainfold, ntree=655, mtry = 78, importance=TRUE, do.trace=TRUE)

# We predict on the test fold
predicted <- predict(model2, newdata = testfold)

# We compare with the observed values and calculate RMSE
observed  <- testfold$response
mse       <- mean((observed-predicted)^2)

(rmse     <- sqrt(mse))
test_rmse <- rmse
submission <- tibble(predicted)
```

> Q use cross validation to examine how accurate are the predictions you can make

```{r CV, eval=FALSE, include=FALSE}
# Function for calculating the 10-fold CV error
cross_validation <- function(dataset, k=10){
  
  cvfolds <- cut(seq_len(nrow(dataset)), breaks = k, labels = FALSE)  # Generate breaks
  cvfolds <- sample(cvfolds)                                      # Randomize breaks

  observed  <- dataset$response                                       # Get observed values
  predicted <- rep(NA, nrow(dataset))                             # Empty vector for predicted values
  
  for (i in 1:k){                                                 # For each fold
    rows      <- which(cvfolds==i)                                # Get values belonging to that fold
    testdata  <- dataset[rows,]                                   # Get test values
    traindata <- dataset[-rows,]                                  # Get training values
    fit       <- lm(myForm, data=traindata)                       # Fit the model with training data
    tmp       <- predict(fit,newdata = testdata)                  # Predict on test data
    predicted[rows] <- tmp                                        # Add predictions to vector
  }
  
  rmse_cv <- sqrt(mean((observed-predicted)^2))        # Calculate CV RMSE

    fit        <- lm(myForm, data=dataset)                          # Fit the model with all the dataset
  observed   <- dataset$response                                      # Get observed values
  predicted  <- predict(fit,newdata = dataset)                    # Get predicted values
  rmse_train <- sqrt(mean((observed-predicted)^2))     # Calculate training RMSE
  
  return(c(rmse_cv, rmse_train))                                  # Return CV and training RMSEs
}

pd <- tibble(run=1:5)                                             # Empty tibble for five different runs

for (i in pd$run) {                                               # For each of the runs
  set.seed(i)
  r <- cross_validation(als_train, 10)                             # Run the CV analysis
  pd$rmse_cv[i] <- r[1]                                           # Save the CV RMSE
  pd$rmse_train <- r[2]                                           # Save the training RMSE
  cat("Run", i, "\n")                                             # Print status
  flush.console()
}

knitr::kable(pd)
```

```{r}
# Always make the same split
set.seed(0)

# We use 80% for training and 20% for evaluation
trainfold <- als_train %>% sample_frac(size=0.80)
testfold  <- setdiff(als_train, trainfold)

# We fit our model to the training fold
#fit <-  lm(formula  = myForm, data = trainfold)

# We predict on the test fold
predicted <- FNN::knn.reg(as.data.frame(trainfold[-1]), 
                test=as.data.frame(testfold[-1]), as.data.frame(trainfold[1]),
                k = 6)

# We compare with the observed values and calculate RMSE
observed  <- testfold$response
mse       <- mean((observed-predicted$pred)^2)
(rmse     <- sqrt(mse))

test_rmse <- rmse


```

# Submitting your answer

The following code will give us

-   your chosen team name
-   the name of the people on the team
-   your estimated RMSE (from train/test or CV or similar)
-   your predictions

Please edit the values below .

The filename of the output will be automated als_progression.TEAMNAME.rds

Please - do not use space or funny letters in your team name.

```{r}

team_name        <- "Team Bulbasaur"
team_people      <- c("Max Gubert", "Rikke Jensen", "Marc Solé")
team_error_rate  <- test_rmse
team_predictions <- submission

#
# Always run this code
# If it fails you have done something wrong!
#
# Extract the columns needed
team_predictions <- team_predictions %>% select(predicted)

# Save all the stuff in one object
write_rds(x = list(team_name, team_people, team_error_rate, team_predictions), 
          file = paste("als_progression.", team_name, ".rds", sep=""))

```

# Checking format of all saved objects

```{r}
files   <- Sys.glob("als_progression.*.rds")
results <- tibble(filename = files, team_name=NA, team_people=NA, team_rmse=NA,n=NA, mean=NA)


  x <- read_rds(files)
  results$team_name        <- x[[1]]
  results$team_people      <- paste(x[[2]], collapse=",", sep=" ")
  results$team_rmse       <- x[[3]]
  y                           <- x[[4]]
  results$n                   <- nrow(y)
  results$mean                <- mean(y$predicted, na.rm = T)

rm(x,y)

results %>% select(-filename)
```
